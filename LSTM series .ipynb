{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading \n",
    "import pandas as pd \n",
    "\n",
    "# Array math \n",
    "import numpy as np\n",
    "\n",
    "# Dates \n",
    "import datetime\n",
    "\n",
    "# Ploting \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf \n",
    "\n",
    "# Keras API \n",
    "from tensorflow import keras\n",
    "\n",
    "# Deep learning \n",
    "from keras.models import Input, Model, Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read municipality data in 0.33 seconds\n",
      "Rows read: 13912\n",
      "Read patient data in 0.65 seconds\n",
      "Rows read: 35911\n",
      "Data saved in data/2020-11-16\n"
     ]
    }
   ],
   "source": [
    "# Downloading data \n",
    "!python3 dataDownload.py\n",
    "\n",
    "# Creating tidy data\n",
    "!python3 createTidyData.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data \n",
    "d = pd.read_csv('data/tidy_data.csv')\n",
    "\n",
    "# Sorting by date \n",
    "d['day'] = [datetime.datetime.strptime(x, '%Y-%m-%d').date() for x in d['day']]\n",
    "d.sort_values('day', inplace=True)\n",
    "\n",
    "# Leaving only a subset of features\n",
    "dLSTM = d.drop('day', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLSTM = series_to_supervised(dLSTM, n_in=3)\n",
    "\n",
    "# Getting the t1 names \n",
    "t1 = [x for x in dLSTM.columns if '(t)' in x]\n",
    "\n",
    "# Leaving only var 1\n",
    "t1 = list(set(t1) - set(['var1(t)']))\n",
    "\n",
    "# Droping the current info \n",
    "dLSTM = dLSTM.drop(t1, axis=1)\n",
    "\n",
    "# Reseting the index \n",
    "dLSTM.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245, 306) (245,) (3, 306) (3,)\n"
     ]
    }
   ],
   "source": [
    "# Defining how many last day data to use in validation \n",
    "n_last = 3\n",
    "\n",
    "# Spliting to training and validation sets \n",
    "test = dLSTM.tail(n_last)\n",
    "train = dLSTM[~dLSTM.index.isin(test.index)]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train.drop('var1(t)', axis=1), train['var1(t)']\n",
    "test_X, test_y = test.drop('var1(t)', axis=1), test['var1(t)']\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "#train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "#test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "inputRegression = Input(shape=(train_X.shape[1], ))\n",
    "\n",
    "# Adding one output linear neuron\n",
    "neuron = Dense(1, activation='linear')(inputRegression)\n",
    "\n",
    "# Defining the model\n",
    "model = Model(inputRegression, neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 306)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 307       \n",
      "=================================================================\n",
      "Total params: 307\n",
      "Trainable params: 307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 255.8521 - val_loss: 479.1767\n",
      "Epoch 2/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 245.5866 - val_loss: 525.5630\n",
      "Epoch 3/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 235.9192 - val_loss: 567.8831\n",
      "Epoch 4/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.3331 - val_loss: 607.9568\n",
      "Epoch 5/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 217.5976 - val_loss: 652.0738\n",
      "Epoch 6/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 208.9959 - val_loss: 691.7997\n",
      "Epoch 7/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 201.9825 - val_loss: 731.2458\n",
      "Epoch 8/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 195.8035 - val_loss: 757.4202\n",
      "Epoch 9/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 190.9317 - val_loss: 784.9092\n",
      "Epoch 10/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 186.2455 - val_loss: 810.0076\n",
      "Epoch 11/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 182.3074 - val_loss: 828.4539\n",
      "Epoch 12/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 179.3111 - val_loss: 845.1695\n",
      "Epoch 13/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 176.6053 - val_loss: 853.2386\n",
      "Epoch 14/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 174.5053 - val_loss: 860.0130\n",
      "Epoch 15/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 172.4782 - val_loss: 869.5972\n",
      "Epoch 16/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 170.2250 - val_loss: 873.0901\n",
      "Epoch 17/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 168.1329 - val_loss: 881.9766\n",
      "Epoch 18/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 166.2920 - val_loss: 889.6541\n",
      "Epoch 19/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 164.6933 - val_loss: 895.8291\n",
      "Epoch 20/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 163.1400 - val_loss: 892.0826\n",
      "Epoch 21/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 161.7622 - val_loss: 888.1465\n",
      "Epoch 22/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 160.4855 - val_loss: 889.0947\n",
      "Epoch 23/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.0909 - val_loss: 875.5500\n",
      "Epoch 24/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 157.6842 - val_loss: 865.1843\n",
      "Epoch 25/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 156.3473 - val_loss: 856.0861\n",
      "Epoch 26/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.0088 - val_loss: 858.1942\n",
      "Epoch 27/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 153.6672 - val_loss: 848.0417\n",
      "Epoch 28/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 152.2723 - val_loss: 847.3635\n",
      "Epoch 29/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 150.8750 - val_loss: 838.1608\n",
      "Epoch 30/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.5562 - val_loss: 831.6792\n",
      "Epoch 31/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 148.3875 - val_loss: 816.3066\n",
      "Epoch 32/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.0297 - val_loss: 820.8127\n",
      "Epoch 33/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 145.5509 - val_loss: 808.2950\n",
      "Epoch 34/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 144.2042 - val_loss: 799.9684\n",
      "Epoch 35/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 142.9829 - val_loss: 787.4769\n",
      "Epoch 36/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 141.5991 - val_loss: 786.1735\n",
      "Epoch 37/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2479 - val_loss: 771.5318\n",
      "Epoch 38/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 138.9510 - val_loss: 760.3374\n",
      "Epoch 39/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 137.6442 - val_loss: 751.1248\n",
      "Epoch 40/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 136.4286 - val_loss: 751.3979\n",
      "Epoch 41/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.0631 - val_loss: 740.9436\n",
      "Epoch 42/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 133.8332 - val_loss: 730.2957\n",
      "Epoch 43/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 132.4743 - val_loss: 722.0593\n",
      "Epoch 44/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 131.2330 - val_loss: 719.2662\n",
      "Epoch 45/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 129.9168 - val_loss: 701.4911\n",
      "Epoch 46/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 128.6096 - val_loss: 695.5396\n",
      "Epoch 47/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.1789 - val_loss: 681.2270\n",
      "Epoch 48/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 125.9705 - val_loss: 676.4667\n",
      "Epoch 49/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.6453 - val_loss: 664.4065\n",
      "Epoch 50/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3465 - val_loss: 651.4135\n",
      "Epoch 51/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 122.0630 - val_loss: 639.9957\n",
      "Epoch 52/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 120.7081 - val_loss: 626.4231\n",
      "Epoch 53/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 119.5883 - val_loss: 619.4451\n",
      "Epoch 54/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 118.2201 - val_loss: 598.9061\n",
      "Epoch 55/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9063 - val_loss: 585.4746\n",
      "Epoch 56/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5645 - val_loss: 576.5927\n",
      "Epoch 57/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2603 - val_loss: 571.9224\n",
      "Epoch 58/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 113.0368 - val_loss: 567.4987\n",
      "Epoch 59/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 111.7798 - val_loss: 559.8275\n",
      "Epoch 60/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 110.4573 - val_loss: 557.0046\n",
      "Epoch 61/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 109.2666 - val_loss: 544.3141\n",
      "Epoch 62/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9638 - val_loss: 532.4604\n",
      "Epoch 63/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 106.7450 - val_loss: 521.0112\n",
      "Epoch 64/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.4453 - val_loss: 517.6750\n",
      "Epoch 65/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 104.2426 - val_loss: 504.1689\n",
      "Epoch 66/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 103.0093 - val_loss: 498.7877\n",
      "Epoch 67/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7285 - val_loss: 480.9059\n",
      "Epoch 68/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5906 - val_loss: 471.7150\n",
      "Epoch 69/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4639 - val_loss: 470.8234\n",
      "Epoch 70/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 98.4284 - val_loss: 463.3391\n",
      "Epoch 71/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 97.3595 - val_loss: 451.3189\n",
      "Epoch 72/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.3646 - val_loss: 444.7346\n",
      "Epoch 73/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.5039 - val_loss: 442.3021\n",
      "Epoch 74/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.4623 - val_loss: 436.0837\n",
      "Epoch 75/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 93.4390 - val_loss: 423.7472\n",
      "Epoch 76/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 92.4764 - val_loss: 416.8756\n",
      "Epoch 77/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.7174 - val_loss: 410.4448\n",
      "Epoch 78/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.7156 - val_loss: 393.9155\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 89.7659 - val_loss: 382.9238\n",
      "Epoch 80/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.9291 - val_loss: 372.6418\n",
      "Epoch 81/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.0202 - val_loss: 365.6439\n",
      "Epoch 82/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.2772 - val_loss: 365.7392\n",
      "Epoch 83/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 86.2265 - val_loss: 360.5706\n",
      "Epoch 84/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.3554 - val_loss: 345.6881\n",
      "Epoch 85/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 84.5358 - val_loss: 335.1375\n",
      "Epoch 86/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 83.6311 - val_loss: 328.2284\n",
      "Epoch 87/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 82.7913 - val_loss: 321.6120\n",
      "Epoch 88/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9362 - val_loss: 314.0266\n",
      "Epoch 89/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 81.1054 - val_loss: 302.2825\n",
      "Epoch 90/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4256 - val_loss: 293.8564\n",
      "Epoch 91/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 79.5143 - val_loss: 294.7488\n",
      "Epoch 92/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.7604 - val_loss: 278.2627\n",
      "Epoch 93/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.9036 - val_loss: 274.7690\n",
      "Epoch 94/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 77.0402 - val_loss: 264.5591\n",
      "Epoch 95/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 76.2396 - val_loss: 253.5535\n",
      "Epoch 96/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 75.4730 - val_loss: 248.2081\n",
      "Epoch 97/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.7915 - val_loss: 241.1340\n",
      "Epoch 98/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.1240 - val_loss: 229.0482\n",
      "Epoch 99/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.4527 - val_loss: 226.9934\n",
      "Epoch 100/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 72.8086 - val_loss: 224.6095\n",
      "Epoch 101/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 72.2000 - val_loss: 221.9971\n",
      "Epoch 102/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.5946 - val_loss: 220.9451\n",
      "Epoch 103/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 70.9193 - val_loss: 215.2514\n",
      "Epoch 104/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 70.2654 - val_loss: 209.9995\n",
      "Epoch 105/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 69.6232 - val_loss: 206.0212\n",
      "Epoch 106/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.1330 - val_loss: 205.9089\n",
      "Epoch 107/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 68.4826 - val_loss: 199.8111\n",
      "Epoch 108/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.8749 - val_loss: 197.5481\n",
      "Epoch 109/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.2119 - val_loss: 194.5659\n",
      "Epoch 110/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.6182 - val_loss: 191.8994\n",
      "Epoch 111/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.1055 - val_loss: 190.6316\n",
      "Epoch 112/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 65.5732 - val_loss: 186.1766\n",
      "Epoch 113/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.9124 - val_loss: 181.6818\n",
      "Epoch 114/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 64.3335 - val_loss: 178.3952\n",
      "Epoch 115/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 63.7337 - val_loss: 169.9640\n",
      "Epoch 116/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 63.2635 - val_loss: 166.0646\n",
      "Epoch 117/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.8105 - val_loss: 166.9882\n",
      "Epoch 118/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 62.0938 - val_loss: 165.6544\n",
      "Epoch 119/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 61.5640 - val_loss: 161.4838\n",
      "Epoch 120/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 61.0218 - val_loss: 159.9626\n",
      "Epoch 121/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 60.5700 - val_loss: 157.0355\n",
      "Epoch 122/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 59.9756 - val_loss: 150.1092\n",
      "Epoch 123/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 59.4126 - val_loss: 148.2281\n",
      "Epoch 124/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 58.8936 - val_loss: 147.9468\n",
      "Epoch 125/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58.4155 - val_loss: 148.2703\n",
      "Epoch 126/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.9565 - val_loss: 153.0577\n",
      "Epoch 127/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 57.4410 - val_loss: 154.8589\n",
      "Epoch 128/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.0064 - val_loss: 153.0998\n",
      "Epoch 129/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 56.3167 - val_loss: 156.2693\n",
      "Epoch 130/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55.7869 - val_loss: 159.1425\n",
      "Epoch 131/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55.3204 - val_loss: 159.4147\n",
      "Epoch 132/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 54.8646 - val_loss: 162.9993\n",
      "Epoch 133/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 54.4212 - val_loss: 166.6436\n",
      "Epoch 134/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 53.8677 - val_loss: 163.6755\n",
      "Epoch 135/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 53.5750 - val_loss: 168.1708\n",
      "Epoch 136/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 53.0398 - val_loss: 169.3701\n",
      "Epoch 137/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 52.5633 - val_loss: 171.3663\n",
      "Epoch 138/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 52.1059 - val_loss: 171.4577\n",
      "Epoch 139/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 51.6937 - val_loss: 174.1383\n",
      "Epoch 140/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 51.2183 - val_loss: 175.5065\n",
      "Epoch 141/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 50.7650 - val_loss: 177.2769\n",
      "Epoch 142/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 50.2640 - val_loss: 181.2225\n",
      "Epoch 143/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 49.8908 - val_loss: 182.2373\n",
      "Epoch 144/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 49.5193 - val_loss: 183.0464\n",
      "Epoch 145/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 49.1344 - val_loss: 184.3913\n",
      "Epoch 146/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 48.7690 - val_loss: 187.1433\n",
      "Epoch 147/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 48.4010 - val_loss: 188.0842\n",
      "Epoch 148/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 48.0201 - val_loss: 189.6214\n",
      "Epoch 149/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 47.7282 - val_loss: 192.9419\n",
      "Epoch 150/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 47.3115 - val_loss: 191.8523\n",
      "Epoch 151/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 46.9298 - val_loss: 195.1882\n",
      "Epoch 152/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 46.5630 - val_loss: 197.6673\n",
      "Epoch 153/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 46.2098 - val_loss: 197.1062\n",
      "Epoch 154/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 45.8541 - val_loss: 200.9234\n",
      "Epoch 155/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 45.4650 - val_loss: 202.3560\n",
      "Epoch 156/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 45.1398 - val_loss: 204.0823\n",
      "Epoch 157/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 44.7889 - val_loss: 208.3840\n",
      "Epoch 158/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 44.4205 - val_loss: 207.4366\n",
      "Epoch 159/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 44.1253 - val_loss: 209.8715\n",
      "Epoch 160/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 43.7516 - val_loss: 212.8532\n",
      "Epoch 161/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 43.3806 - val_loss: 212.3442\n",
      "Epoch 162/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 43.1262 - val_loss: 213.9923\n",
      "Epoch 163/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 42.7942 - val_loss: 217.2714\n",
      "Epoch 164/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 42.7233 - val_loss: 216.6320\n",
      "Epoch 165/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 42.1158 - val_loss: 227.8504\n",
      "Epoch 166/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 41.9600 - val_loss: 228.4066\n",
      "Epoch 167/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 41.5919 - val_loss: 229.3309\n",
      "Epoch 168/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 41.3578 - val_loss: 239.7430\n",
      "Epoch 169/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 41.1751 - val_loss: 248.5380\n",
      "Epoch 170/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 40.8031 - val_loss: 251.2666\n",
      "Epoch 171/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 40.6064 - val_loss: 256.2123\n",
      "Epoch 172/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 40.3861 - val_loss: 262.2053\n",
      "Epoch 173/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 40.1800 - val_loss: 267.2892\n",
      "Epoch 174/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 39.8488 - val_loss: 273.2242\n",
      "Epoch 175/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 39.6739 - val_loss: 276.0840\n",
      "Epoch 176/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 39.6494 - val_loss: 284.6448\n",
      "Epoch 177/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 39.4135 - val_loss: 286.4961\n",
      "Epoch 178/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 39.2764 - val_loss: 291.7309\n",
      "Epoch 179/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 39.0232 - val_loss: 297.9494\n",
      "Epoch 180/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 38.9044 - val_loss: 301.4308\n",
      "Epoch 181/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 38.7394 - val_loss: 306.4316\n",
      "Epoch 182/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 38.5698 - val_loss: 311.1821\n",
      "Epoch 183/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 38.4216 - val_loss: 316.7793\n",
      "Epoch 184/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 38.3531 - val_loss: 319.4931\n",
      "Epoch 185/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 38.1951 - val_loss: 324.3413\n",
      "Epoch 186/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 38.0154 - val_loss: 331.5623\n",
      "Epoch 187/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.8643 - val_loss: 336.6325\n",
      "Epoch 188/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.7592 - val_loss: 337.8923\n",
      "Epoch 189/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.5513 - val_loss: 346.8521\n",
      "Epoch 190/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.4913 - val_loss: 349.3564\n",
      "Epoch 191/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.3633 - val_loss: 354.5101\n",
      "Epoch 192/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.2212 - val_loss: 357.1629\n",
      "Epoch 193/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.2007 - val_loss: 358.7822\n",
      "Epoch 194/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.2205 - val_loss: 366.1995\n",
      "Epoch 195/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.1265 - val_loss: 362.5980\n",
      "Epoch 196/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.0183 - val_loss: 370.8522\n",
      "Epoch 197/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36.9425 - val_loss: 369.5016\n",
      "Epoch 198/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.8763 - val_loss: 372.4899\n",
      "Epoch 199/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.8363 - val_loss: 374.7788\n",
      "Epoch 200/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.7165 - val_loss: 377.5102\n",
      "Epoch 201/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.7352 - val_loss: 382.3413\n",
      "Epoch 202/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.7619 - val_loss: 382.7594\n",
      "Epoch 203/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.6073 - val_loss: 390.8522\n",
      "Epoch 204/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.6065 - val_loss: 384.0309\n",
      "Epoch 205/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36.5666 - val_loss: 391.2771\n",
      "Epoch 206/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.4638 - val_loss: 390.9066\n",
      "Epoch 207/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36.6790 - val_loss: 397.1676\n",
      "Epoch 208/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.5495 - val_loss: 394.1249\n",
      "Epoch 209/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.3780 - val_loss: 395.1557\n",
      "Epoch 210/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.3641 - val_loss: 398.9413\n",
      "Epoch 211/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.3896 - val_loss: 404.4409\n",
      "Epoch 212/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.3251 - val_loss: 401.3045\n",
      "Epoch 213/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.3130 - val_loss: 407.0626\n",
      "Epoch 214/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.3918 - val_loss: 406.2975\n",
      "Epoch 215/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.2711 - val_loss: 405.4431\n",
      "Epoch 216/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.2560 - val_loss: 412.3716\n",
      "Epoch 217/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.3480 - val_loss: 410.9086\n",
      "Epoch 218/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36.1984 - val_loss: 417.0128\n",
      "Epoch 219/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36.1910 - val_loss: 417.6134\n",
      "Epoch 220/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.3146 - val_loss: 420.9995\n",
      "Epoch 221/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.1195 - val_loss: 417.1181\n",
      "Epoch 222/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.0939 - val_loss: 423.0212\n",
      "Epoch 223/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.1128 - val_loss: 425.3488\n",
      "Epoch 224/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.0561 - val_loss: 425.7137\n",
      "Epoch 225/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.0474 - val_loss: 434.0122\n",
      "Epoch 226/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.0043 - val_loss: 433.8343\n",
      "Epoch 227/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.9463 - val_loss: 435.7327\n",
      "Epoch 228/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.9475 - val_loss: 437.9397\n",
      "Epoch 229/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.9468 - val_loss: 438.1678\n",
      "Epoch 230/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.9267 - val_loss: 441.2161\n",
      "Epoch 231/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.9571 - val_loss: 444.5537\n",
      "Epoch 232/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36.1441 - val_loss: 450.7248\n",
      "Epoch 233/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.8352 - val_loss: 443.9127\n",
      "Epoch 234/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 36.1794 - val_loss: 452.6187\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 35.8653 - val_loss: 453.1547\n",
      "Epoch 236/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.7991 - val_loss: 458.1469\n",
      "Epoch 237/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.8031 - val_loss: 460.6352\n",
      "Epoch 238/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.8669 - val_loss: 463.0015\n",
      "Epoch 239/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.8178 - val_loss: 464.0095\n",
      "Epoch 240/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.6583 - val_loss: 467.0445\n",
      "Epoch 241/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.7237 - val_loss: 467.0114\n",
      "Epoch 242/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.7819 - val_loss: 471.5164\n",
      "Epoch 243/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.7992 - val_loss: 472.8279\n",
      "Epoch 244/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.7680 - val_loss: 475.4237\n",
      "Epoch 245/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.7020 - val_loss: 475.0891\n",
      "Epoch 246/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.6054 - val_loss: 478.9893\n",
      "Epoch 247/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.7010 - val_loss: 476.9963\n",
      "Epoch 248/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36.0266 - val_loss: 479.9192\n",
      "Epoch 249/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.6647 - val_loss: 481.2494\n",
      "Epoch 250/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.6231 - val_loss: 483.3137\n",
      "Epoch 251/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.5991 - val_loss: 480.2626\n",
      "Epoch 252/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.5273 - val_loss: 483.2854\n",
      "Epoch 253/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.5050 - val_loss: 485.0778\n",
      "Epoch 254/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.4823 - val_loss: 484.5268\n",
      "Epoch 255/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.5309 - val_loss: 484.4685\n",
      "Epoch 256/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.6670 - val_loss: 484.9067\n",
      "Epoch 257/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.5635 - val_loss: 483.9254\n",
      "Epoch 258/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.6871 - val_loss: 485.9558\n",
      "Epoch 259/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.9070 - val_loss: 489.3871\n",
      "Epoch 260/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.6104 - val_loss: 484.8431\n",
      "Epoch 261/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.4657 - val_loss: 490.4017\n",
      "Epoch 262/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.5921 - val_loss: 489.8173\n",
      "Epoch 263/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.4386 - val_loss: 488.3046\n",
      "Epoch 264/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.4720 - val_loss: 491.0872\n",
      "Epoch 265/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.4596 - val_loss: 488.8827\n",
      "Epoch 266/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.3814 - val_loss: 491.0710\n",
      "Epoch 267/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.3908 - val_loss: 491.6681\n",
      "Epoch 268/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.4347 - val_loss: 489.9241\n",
      "Epoch 269/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.4459 - val_loss: 492.9972\n",
      "Epoch 270/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.3866 - val_loss: 491.3326\n",
      "Epoch 271/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.6873 - val_loss: 493.0161\n",
      "Epoch 272/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.5013 - val_loss: 490.9462\n",
      "Epoch 273/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.3650 - val_loss: 492.0169\n",
      "Epoch 274/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.4498 - val_loss: 493.7999\n",
      "Epoch 275/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.4159 - val_loss: 494.4904\n",
      "Epoch 276/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.4332 - val_loss: 496.2124\n",
      "Epoch 277/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.3435 - val_loss: 497.1841\n",
      "Epoch 278/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.3695 - val_loss: 495.2776\n",
      "Epoch 279/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.3413 - val_loss: 497.6869\n",
      "Epoch 280/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2643 - val_loss: 494.8215\n",
      "Epoch 281/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2937 - val_loss: 494.3711\n",
      "Epoch 282/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.3394 - val_loss: 497.4847\n",
      "Epoch 283/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.4057 - val_loss: 496.1687\n",
      "Epoch 284/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2789 - val_loss: 492.8162\n",
      "Epoch 285/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2249 - val_loss: 497.3695\n",
      "Epoch 286/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2807 - val_loss: 494.8336\n",
      "Epoch 287/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2728 - val_loss: 502.0504\n",
      "Epoch 288/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.2492 - val_loss: 496.8480\n",
      "Epoch 289/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2184 - val_loss: 501.4306\n",
      "Epoch 290/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2471 - val_loss: 495.7693\n",
      "Epoch 291/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.3932 - val_loss: 497.6514\n",
      "Epoch 292/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1983 - val_loss: 499.9795\n",
      "Epoch 293/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.3802 - val_loss: 496.1729\n",
      "Epoch 294/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1640 - val_loss: 503.2418\n",
      "Epoch 295/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2947 - val_loss: 495.6250\n",
      "Epoch 296/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1812 - val_loss: 503.4684\n",
      "Epoch 297/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1943 - val_loss: 501.0675\n",
      "Epoch 298/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.2249 - val_loss: 503.7253\n",
      "Epoch 299/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2152 - val_loss: 500.5873\n",
      "Epoch 300/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2341 - val_loss: 506.2651\n",
      "Epoch 301/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2186 - val_loss: 501.4001\n",
      "Epoch 302/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2128 - val_loss: 504.5494\n",
      "Epoch 303/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2241 - val_loss: 504.4449\n",
      "Epoch 304/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1558 - val_loss: 502.3236\n",
      "Epoch 305/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1586 - val_loss: 504.6236\n",
      "Epoch 306/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.1202 - val_loss: 501.7110\n",
      "Epoch 307/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0725 - val_loss: 503.9278\n",
      "Epoch 308/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.1816 - val_loss: 501.8893\n",
      "Epoch 309/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0851 - val_loss: 505.8444\n",
      "Epoch 310/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.2054 - val_loss: 507.9937\n",
      "Epoch 311/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0627 - val_loss: 501.1579\n",
      "Epoch 312/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0000 - val_loss: 506.5745\n",
      "Epoch 313/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 35.0936 - val_loss: 510.4900\n",
      "Epoch 314/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1135 - val_loss: 508.3358\n",
      "Epoch 315/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0922 - val_loss: 509.2301\n",
      "Epoch 316/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1149 - val_loss: 508.6073\n",
      "Epoch 317/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9907 - val_loss: 512.8477\n",
      "Epoch 318/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0396 - val_loss: 508.9825\n",
      "Epoch 319/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0773 - val_loss: 512.0451\n",
      "Epoch 320/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1278 - val_loss: 512.9446\n",
      "Epoch 321/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0207 - val_loss: 508.2820\n",
      "Epoch 322/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9552 - val_loss: 514.9894\n",
      "Epoch 323/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35.1402 - val_loss: 513.7850\n",
      "Epoch 324/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1108 - val_loss: 517.3501\n",
      "Epoch 325/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1517 - val_loss: 515.5715\n",
      "Epoch 326/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9771 - val_loss: 514.6926\n",
      "Epoch 327/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9829 - val_loss: 513.5534\n",
      "Epoch 328/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9162 - val_loss: 517.6733\n",
      "Epoch 329/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0047 - val_loss: 515.7072\n",
      "Epoch 330/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9968 - val_loss: 517.4102\n",
      "Epoch 331/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9180 - val_loss: 514.1296\n",
      "Epoch 332/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.8679 - val_loss: 518.5965\n",
      "Epoch 333/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9147 - val_loss: 515.6931\n",
      "Epoch 334/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.8397 - val_loss: 512.5580\n",
      "Epoch 335/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9013 - val_loss: 514.4043\n",
      "Epoch 336/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9437 - val_loss: 514.9665\n",
      "Epoch 337/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.8436 - val_loss: 514.1359\n",
      "Epoch 338/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.8875 - val_loss: 518.1048\n",
      "Epoch 339/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.8380 - val_loss: 513.1419\n",
      "Epoch 340/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.8418 - val_loss: 514.8924\n",
      "Epoch 341/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.8103 - val_loss: 518.8112\n",
      "Epoch 342/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.0319 - val_loss: 519.8433\n",
      "Epoch 343/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.8242 - val_loss: 516.1648\n",
      "Epoch 344/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7714 - val_loss: 514.4428\n",
      "Epoch 345/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7780 - val_loss: 515.4772\n",
      "Epoch 346/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7867 - val_loss: 517.8806\n",
      "Epoch 347/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.8392 - val_loss: 513.2193\n",
      "Epoch 348/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7675 - val_loss: 516.0692\n",
      "Epoch 349/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7703 - val_loss: 515.1094\n",
      "Epoch 350/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7570 - val_loss: 517.1883\n",
      "Epoch 351/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7628 - val_loss: 521.8621\n",
      "Epoch 352/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7757 - val_loss: 520.6089\n",
      "Epoch 353/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7839 - val_loss: 520.7555\n",
      "Epoch 354/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7845 - val_loss: 520.7195\n",
      "Epoch 355/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7221 - val_loss: 519.8304\n",
      "Epoch 356/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7180 - val_loss: 525.2372\n",
      "Epoch 357/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7943 - val_loss: 519.4902\n",
      "Epoch 358/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7733 - val_loss: 523.6004\n",
      "Epoch 359/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.6702 - val_loss: 520.8494\n",
      "Epoch 360/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7946 - val_loss: 521.7952\n",
      "Epoch 361/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7419 - val_loss: 520.5939\n",
      "Epoch 362/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7069 - val_loss: 518.6104\n",
      "Epoch 363/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.9065 - val_loss: 526.4798\n",
      "Epoch 364/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.6848 - val_loss: 519.2783\n",
      "Epoch 365/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.6418 - val_loss: 520.8494\n",
      "Epoch 366/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.6517 - val_loss: 520.2713\n",
      "Epoch 367/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.6677 - val_loss: 520.3395\n",
      "Epoch 368/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.6287 - val_loss: 526.5674\n",
      "Epoch 369/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.9450 - val_loss: 527.0823\n",
      "Epoch 370/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.6255 - val_loss: 524.2808\n",
      "Epoch 371/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.5855 - val_loss: 525.8508\n",
      "Epoch 372/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.6263 - val_loss: 525.3306\n",
      "Epoch 373/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.6012 - val_loss: 525.8029\n",
      "Epoch 374/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5707 - val_loss: 524.6152\n",
      "Epoch 375/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5512 - val_loss: 523.6635\n",
      "Epoch 376/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5733 - val_loss: 525.3547\n",
      "Epoch 377/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.5833 - val_loss: 523.8426\n",
      "Epoch 378/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5821 - val_loss: 525.9324\n",
      "Epoch 379/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.6127 - val_loss: 527.0721\n",
      "Epoch 380/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.5156 - val_loss: 523.1144\n",
      "Epoch 381/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.6269 - val_loss: 530.4155\n",
      "Epoch 382/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5987 - val_loss: 527.6548\n",
      "Epoch 383/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5433 - val_loss: 526.5081\n",
      "Epoch 384/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.5535 - val_loss: 528.3809\n",
      "Epoch 385/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5293 - val_loss: 528.5971\n",
      "Epoch 386/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5320 - val_loss: 525.4460\n",
      "Epoch 387/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7401 - val_loss: 528.4677\n",
      "Epoch 388/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.5542 - val_loss: 525.8594\n",
      "Epoch 389/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.5129 - val_loss: 529.6125\n",
      "Epoch 390/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.4605 - val_loss: 526.0693\n",
      "Epoch 391/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 34.6578 - val_loss: 532.0632\n",
      "Epoch 392/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.6256 - val_loss: 525.5301\n",
      "Epoch 393/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.4583 - val_loss: 529.3611\n",
      "Epoch 394/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7239 - val_loss: 525.3400\n",
      "Epoch 395/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.6156 - val_loss: 532.1088\n",
      "Epoch 396/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.4106 - val_loss: 523.5723\n",
      "Epoch 397/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.7097 - val_loss: 532.4039\n",
      "Epoch 398/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.7800 - val_loss: 528.0121\n",
      "Epoch 399/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.5217 - val_loss: 536.6575\n",
      "Epoch 400/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.4993 - val_loss: 534.9342\n",
      "Epoch 401/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.4136 - val_loss: 534.3711\n",
      "Epoch 402/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.4050 - val_loss: 537.0496\n",
      "Epoch 403/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.3958 - val_loss: 532.9875\n",
      "Epoch 404/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.3784 - val_loss: 538.3016\n",
      "Epoch 405/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.4639 - val_loss: 536.8030\n",
      "Epoch 406/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.3574 - val_loss: 539.4473\n",
      "Epoch 407/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.4902 - val_loss: 538.4589\n",
      "Epoch 408/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.3886 - val_loss: 536.5740\n",
      "Epoch 409/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.5132 - val_loss: 538.4200\n",
      "Epoch 410/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.2891 - val_loss: 533.6934\n",
      "Epoch 411/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.4018 - val_loss: 534.4377\n",
      "Epoch 412/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.3286 - val_loss: 535.6763\n",
      "Epoch 413/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.2979 - val_loss: 533.3873\n",
      "Epoch 414/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.4474 - val_loss: 534.4656\n",
      "Epoch 415/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.3030 - val_loss: 532.9391\n",
      "Epoch 416/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.2908 - val_loss: 532.8254\n",
      "Epoch 417/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.3258 - val_loss: 537.0337\n",
      "Epoch 418/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.3686 - val_loss: 543.6522\n",
      "Epoch 419/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.3389 - val_loss: 545.7414\n",
      "Epoch 420/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.3213 - val_loss: 542.0222\n",
      "Epoch 421/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.2434 - val_loss: 544.5677\n",
      "Epoch 422/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.3201 - val_loss: 547.1033\n",
      "Epoch 423/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.3285 - val_loss: 548.0974\n",
      "Epoch 424/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.3733 - val_loss: 539.5422\n",
      "Epoch 425/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.2282 - val_loss: 545.4360\n",
      "Epoch 426/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.3238 - val_loss: 541.7894\n",
      "Epoch 427/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.4429 - val_loss: 539.7737\n",
      "Epoch 428/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1834 - val_loss: 540.4185\n",
      "Epoch 429/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.2063 - val_loss: 540.7288\n",
      "Epoch 430/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.2298 - val_loss: 540.6068\n",
      "Epoch 431/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.2004 - val_loss: 541.6863\n",
      "Epoch 432/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1439 - val_loss: 541.4323\n",
      "Epoch 433/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.2125 - val_loss: 541.7527\n",
      "Epoch 434/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1449 - val_loss: 543.2676\n",
      "Epoch 435/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.2418 - val_loss: 544.1870\n",
      "Epoch 436/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1489 - val_loss: 543.8704\n",
      "Epoch 437/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.2120 - val_loss: 547.9457\n",
      "Epoch 438/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1938 - val_loss: 541.4543\n",
      "Epoch 439/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1897 - val_loss: 542.7080\n",
      "Epoch 440/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1573 - val_loss: 542.7374\n",
      "Epoch 441/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1616 - val_loss: 546.2939\n",
      "Epoch 442/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1328 - val_loss: 539.6480\n",
      "Epoch 443/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1300 - val_loss: 545.0323\n",
      "Epoch 444/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1585 - val_loss: 543.1075\n",
      "Epoch 445/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0643 - val_loss: 546.6601\n",
      "Epoch 446/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1237 - val_loss: 547.4267\n",
      "Epoch 447/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.0391 - val_loss: 544.3989\n",
      "Epoch 448/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1421 - val_loss: 546.0244\n",
      "Epoch 449/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1653 - val_loss: 550.7313\n",
      "Epoch 450/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1423 - val_loss: 548.6491\n",
      "Epoch 451/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.3076 - val_loss: 550.0300\n",
      "Epoch 452/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9834 - val_loss: 548.3809\n",
      "Epoch 453/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.2434 - val_loss: 553.4761\n",
      "Epoch 454/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.0966 - val_loss: 548.8763\n",
      "Epoch 455/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1559 - val_loss: 550.7537\n",
      "Epoch 456/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0637 - val_loss: 552.6025\n",
      "Epoch 457/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0827 - val_loss: 548.9229\n",
      "Epoch 458/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0123 - val_loss: 547.0809\n",
      "Epoch 459/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0474 - val_loss: 550.7796\n",
      "Epoch 460/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.0788 - val_loss: 548.2114\n",
      "Epoch 461/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.9659 - val_loss: 549.3413\n",
      "Epoch 462/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9806 - val_loss: 556.6060\n",
      "Epoch 463/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9259 - val_loss: 546.3776\n",
      "Epoch 464/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.1856 - val_loss: 551.9498\n",
      "Epoch 465/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0197 - val_loss: 548.6255\n",
      "Epoch 466/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.9762 - val_loss: 553.7702\n",
      "Epoch 467/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0079 - val_loss: 553.5083\n",
      "Epoch 468/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0637 - val_loss: 553.5978\n",
      "Epoch 469/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1711 - val_loss: 551.9729\n",
      "Epoch 470/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9371 - val_loss: 554.4899\n",
      "Epoch 471/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9188 - val_loss: 554.8115\n",
      "Epoch 472/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9008 - val_loss: 551.9765\n",
      "Epoch 473/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9148 - val_loss: 552.8365\n",
      "Epoch 474/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.8877 - val_loss: 553.5753\n",
      "Epoch 475/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9602 - val_loss: 553.3647\n",
      "Epoch 476/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.1238 - val_loss: 558.8698\n",
      "Epoch 477/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34.0783 - val_loss: 557.8411\n",
      "Epoch 478/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.8459 - val_loss: 555.3179\n",
      "Epoch 479/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.8403 - val_loss: 558.3685\n",
      "Epoch 480/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9195 - val_loss: 560.4532\n",
      "Epoch 481/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9980 - val_loss: 562.4235\n",
      "Epoch 482/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.8073 - val_loss: 557.7855\n",
      "Epoch 483/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.8129 - val_loss: 561.3277\n",
      "Epoch 484/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.8461 - val_loss: 560.7878\n",
      "Epoch 485/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.8233 - val_loss: 563.3348\n",
      "Epoch 486/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.7853 - val_loss: 560.8127\n",
      "Epoch 487/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7985 - val_loss: 563.5603\n",
      "Epoch 488/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.9212 - val_loss: 564.6337\n",
      "Epoch 489/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7774 - val_loss: 557.3798\n",
      "Epoch 490/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7730 - val_loss: 559.6251\n",
      "Epoch 491/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.8125 - val_loss: 561.7027\n",
      "Epoch 492/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.6860 - val_loss: 556.0369\n",
      "Epoch 493/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.8373 - val_loss: 564.2990\n",
      "Epoch 494/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.8216 - val_loss: 560.5258\n",
      "Epoch 495/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7188 - val_loss: 559.1191\n",
      "Epoch 496/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7783 - val_loss: 560.4284\n",
      "Epoch 497/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.8577 - val_loss: 567.7805\n",
      "Epoch 498/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.9818 - val_loss: 562.4031\n",
      "Epoch 499/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7262 - val_loss: 566.3566\n",
      "Epoch 500/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.6880 - val_loss: 564.1975\n",
      "Epoch 501/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.6777 - val_loss: 567.9595\n",
      "Epoch 502/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7153 - val_loss: 565.5742\n",
      "Epoch 503/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7000 - val_loss: 563.6615\n",
      "Epoch 504/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.8007 - val_loss: 567.7122\n",
      "Epoch 505/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.6688 - val_loss: 570.8054\n",
      "Epoch 506/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.6611 - val_loss: 569.3950\n",
      "Epoch 507/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.6203 - val_loss: 567.1958\n",
      "Epoch 508/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7634 - val_loss: 574.7889\n",
      "Epoch 509/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7341 - val_loss: 569.1214\n",
      "Epoch 510/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.8802 - val_loss: 568.9272\n",
      "Epoch 511/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.6503 - val_loss: 569.3010\n",
      "Epoch 512/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.6202 - val_loss: 568.7418\n",
      "Epoch 513/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.5872 - val_loss: 568.7805\n",
      "Epoch 514/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.6039 - val_loss: 569.3611\n",
      "Epoch 515/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.6933 - val_loss: 568.7076\n",
      "Epoch 516/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.6259 - val_loss: 566.4376\n",
      "Epoch 517/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5896 - val_loss: 571.2350\n",
      "Epoch 518/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7550 - val_loss: 568.8885\n",
      "Epoch 519/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5848 - val_loss: 573.5391\n",
      "Epoch 520/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.6022 - val_loss: 571.5185\n",
      "Epoch 521/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.6324 - val_loss: 573.3250\n",
      "Epoch 522/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5555 - val_loss: 567.3929\n",
      "Epoch 523/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5624 - val_loss: 571.2534\n",
      "Epoch 524/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5856 - val_loss: 569.9745\n",
      "Epoch 525/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.5543 - val_loss: 571.5226\n",
      "Epoch 526/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5188 - val_loss: 571.7215\n",
      "Epoch 527/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5528 - val_loss: 565.0408\n",
      "Epoch 528/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5260 - val_loss: 569.4044\n",
      "Epoch 529/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.5367 - val_loss: 568.7791\n",
      "Epoch 530/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5070 - val_loss: 570.5888\n",
      "Epoch 531/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4798 - val_loss: 570.7545\n",
      "Epoch 532/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4583 - val_loss: 569.7331\n",
      "Epoch 533/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4430 - val_loss: 570.2372\n",
      "Epoch 534/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4612 - val_loss: 574.2315\n",
      "Epoch 535/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7088 - val_loss: 572.8915\n",
      "Epoch 536/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5778 - val_loss: 577.5665\n",
      "Epoch 537/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4911 - val_loss: 573.6107\n",
      "Epoch 538/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.4561 - val_loss: 574.2264\n",
      "Epoch 539/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4747 - val_loss: 571.0119\n",
      "Epoch 540/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4777 - val_loss: 571.0313\n",
      "Epoch 541/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.4088 - val_loss: 571.6537\n",
      "Epoch 542/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.5721 - val_loss: 576.9308\n",
      "Epoch 543/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.8551 - val_loss: 574.3505\n",
      "Epoch 544/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5036 - val_loss: 576.2322\n",
      "Epoch 545/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.3935 - val_loss: 574.8050\n",
      "Epoch 546/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4487 - val_loss: 577.1769\n",
      "Epoch 547/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3806 - val_loss: 577.8351\n",
      "Epoch 548/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.3665 - val_loss: 574.1568\n",
      "Epoch 549/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3852 - val_loss: 583.2706\n",
      "Epoch 550/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4635 - val_loss: 572.6663\n",
      "Epoch 551/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3789 - val_loss: 579.2245\n",
      "Epoch 552/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3787 - val_loss: 582.5038\n",
      "Epoch 553/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.3896 - val_loss: 578.9670\n",
      "Epoch 554/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3281 - val_loss: 580.8710\n",
      "Epoch 555/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.3467 - val_loss: 584.0900\n",
      "Epoch 556/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.3536 - val_loss: 578.0202\n",
      "Epoch 557/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.4024 - val_loss: 581.8328\n",
      "Epoch 558/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3652 - val_loss: 581.7695\n",
      "Epoch 559/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.4016 - val_loss: 581.7333\n",
      "Epoch 560/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.2764 - val_loss: 577.7429\n",
      "Epoch 561/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.3314 - val_loss: 575.8411\n",
      "Epoch 562/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3285 - val_loss: 584.1754\n",
      "Epoch 563/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3009 - val_loss: 579.4705\n",
      "Epoch 564/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.2343 - val_loss: 581.3781\n",
      "Epoch 565/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2344 - val_loss: 581.0787\n",
      "Epoch 566/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2149 - val_loss: 580.0760\n",
      "Epoch 567/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2054 - val_loss: 584.6804\n",
      "Epoch 568/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.3289 - val_loss: 582.2171\n",
      "Epoch 569/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.2831 - val_loss: 579.9250\n",
      "Epoch 570/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2589 - val_loss: 583.2155\n",
      "Epoch 571/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2348 - val_loss: 582.8916\n",
      "Epoch 572/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.2452 - val_loss: 585.0716\n",
      "Epoch 573/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2358 - val_loss: 584.4656\n",
      "Epoch 574/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2006 - val_loss: 584.0410\n",
      "Epoch 575/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1452 - val_loss: 584.7997\n",
      "Epoch 576/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.1785 - val_loss: 586.4089\n",
      "Epoch 577/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1877 - val_loss: 584.6668\n",
      "Epoch 578/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.3564 - val_loss: 591.6721\n",
      "Epoch 579/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.5526 - val_loss: 588.8219\n",
      "Epoch 580/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2555 - val_loss: 591.5972\n",
      "Epoch 581/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.3553 - val_loss: 588.8120\n",
      "Epoch 582/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.2088 - val_loss: 591.8610\n",
      "Epoch 583/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1081 - val_loss: 588.7336\n",
      "Epoch 584/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0977 - val_loss: 589.7040\n",
      "Epoch 585/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1798 - val_loss: 586.4067\n",
      "Epoch 586/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1713 - val_loss: 589.4581\n",
      "Epoch 587/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.1186 - val_loss: 587.1800\n",
      "Epoch 588/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.0748 - val_loss: 586.8230\n",
      "Epoch 589/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.1386 - val_loss: 588.7883\n",
      "Epoch 590/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.1769 - val_loss: 587.9968\n",
      "Epoch 591/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0449 - val_loss: 588.0050\n",
      "Epoch 592/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0568 - val_loss: 590.5413\n",
      "Epoch 593/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0726 - val_loss: 591.7239\n",
      "Epoch 594/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0476 - val_loss: 589.3292\n",
      "Epoch 595/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1174 - val_loss: 594.1349\n",
      "Epoch 596/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1045 - val_loss: 590.4839\n",
      "Epoch 597/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0742 - val_loss: 596.5991\n",
      "Epoch 598/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.0398 - val_loss: 592.3148\n",
      "Epoch 599/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1022 - val_loss: 598.0551\n",
      "Epoch 600/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1442 - val_loss: 593.8942\n",
      "Epoch 601/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0822 - val_loss: 593.9819\n",
      "Epoch 602/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.9738 - val_loss: 596.0590\n",
      "Epoch 603/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.0217 - val_loss: 594.9800\n",
      "Epoch 604/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1517 - val_loss: 595.7146\n",
      "Epoch 605/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1658 - val_loss: 595.6857\n",
      "Epoch 606/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9645 - val_loss: 597.1277\n",
      "Epoch 607/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.1040 - val_loss: 597.6070\n",
      "Epoch 608/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.0345 - val_loss: 598.4204\n",
      "Epoch 609/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9248 - val_loss: 593.3745\n",
      "Epoch 610/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.9823 - val_loss: 596.2930\n",
      "Epoch 611/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9284 - val_loss: 599.3210\n",
      "Epoch 612/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9703 - val_loss: 593.6307\n",
      "Epoch 613/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0830 - val_loss: 590.5099\n",
      "Epoch 614/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0156 - val_loss: 598.3997\n",
      "Epoch 615/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9940 - val_loss: 593.0707\n",
      "Epoch 616/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9741 - val_loss: 596.2905\n",
      "Epoch 617/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.8894 - val_loss: 592.9341\n",
      "Epoch 618/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.8720 - val_loss: 595.4033\n",
      "Epoch 619/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9431 - val_loss: 594.5229\n",
      "Epoch 620/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9099 - val_loss: 599.7994\n",
      "Epoch 621/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.9167 - val_loss: 595.8061\n",
      "Epoch 622/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.9512 - val_loss: 600.1833\n",
      "Epoch 623/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1793 - val_loss: 599.8483\n",
      "Epoch 624/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.1585 - val_loss: 601.2781\n",
      "Epoch 625/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 33.0240 - val_loss: 605.2016\n",
      "Epoch 626/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.8286 - val_loss: 601.4169\n",
      "Epoch 627/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.8519 - val_loss: 602.2142\n",
      "Epoch 628/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7973 - val_loss: 601.9637\n",
      "Epoch 629/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.8150 - val_loss: 603.4149\n",
      "Epoch 630/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9167 - val_loss: 605.9830\n",
      "Epoch 631/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.9552 - val_loss: 608.2608\n",
      "Epoch 632/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7419 - val_loss: 602.6706\n",
      "Epoch 633/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7847 - val_loss: 606.9693\n",
      "Epoch 634/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.7819 - val_loss: 605.2625\n",
      "Epoch 635/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9027 - val_loss: 606.4151\n",
      "Epoch 636/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.8069 - val_loss: 603.5224\n",
      "Epoch 637/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7694 - val_loss: 608.9412\n",
      "Epoch 638/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.7322 - val_loss: 608.9099\n",
      "Epoch 639/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.7742 - val_loss: 612.7170\n",
      "Epoch 640/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.8385 - val_loss: 609.0117\n",
      "Epoch 641/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7581 - val_loss: 611.0637\n",
      "Epoch 642/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.7486 - val_loss: 609.5987\n",
      "Epoch 643/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7221 - val_loss: 610.8824\n",
      "Epoch 644/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6834 - val_loss: 609.2604\n",
      "Epoch 645/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7183 - val_loss: 610.7006\n",
      "Epoch 646/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6891 - val_loss: 610.1920\n",
      "Epoch 647/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.8053 - val_loss: 611.5978\n",
      "Epoch 648/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6992 - val_loss: 611.2578\n",
      "Epoch 649/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6685 - val_loss: 608.9294\n",
      "Epoch 650/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6356 - val_loss: 608.4042\n",
      "Epoch 651/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6571 - val_loss: 608.6281\n",
      "Epoch 652/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.8523 - val_loss: 617.5185\n",
      "Epoch 653/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.7478 - val_loss: 610.3416\n",
      "Epoch 654/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6259 - val_loss: 614.2482\n",
      "Epoch 655/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.6879 - val_loss: 613.9664\n",
      "Epoch 656/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.7517 - val_loss: 616.5540\n",
      "Epoch 657/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7065 - val_loss: 613.3403\n",
      "Epoch 658/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7019 - val_loss: 613.6078\n",
      "Epoch 659/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6350 - val_loss: 614.9411\n",
      "Epoch 660/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7400 - val_loss: 614.8300\n",
      "Epoch 661/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.6967 - val_loss: 613.7053\n",
      "Epoch 662/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.6046 - val_loss: 613.7054\n",
      "Epoch 663/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.5963 - val_loss: 613.5721\n",
      "Epoch 664/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6943 - val_loss: 614.4307\n",
      "Epoch 665/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6855 - val_loss: 617.0434\n",
      "Epoch 666/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.5915 - val_loss: 615.8902\n",
      "Epoch 667/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.5059 - val_loss: 612.1377\n",
      "Epoch 668/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.5736 - val_loss: 612.9633\n",
      "Epoch 669/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.5924 - val_loss: 613.9113\n",
      "Epoch 670/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6724 - val_loss: 615.1002\n",
      "Epoch 671/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4902 - val_loss: 613.6545\n",
      "Epoch 672/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.6244 - val_loss: 622.3712\n",
      "Epoch 673/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.5905 - val_loss: 615.6155\n",
      "Epoch 674/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.6850 - val_loss: 616.0503\n",
      "Epoch 675/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.5407 - val_loss: 617.2704\n",
      "Epoch 676/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.5690 - val_loss: 617.2971\n",
      "Epoch 677/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.5260 - val_loss: 617.5803\n",
      "Epoch 678/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4869 - val_loss: 620.3617\n",
      "Epoch 679/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4967 - val_loss: 616.5544\n",
      "Epoch 680/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.7534 - val_loss: 619.3987\n",
      "Epoch 681/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.4595 - val_loss: 617.5608\n",
      "Epoch 682/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4457 - val_loss: 621.2253\n",
      "Epoch 683/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4354 - val_loss: 623.3179\n",
      "Epoch 684/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4476 - val_loss: 623.5859\n",
      "Epoch 685/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4775 - val_loss: 623.6509\n",
      "Epoch 686/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.5303 - val_loss: 621.7546\n",
      "Epoch 687/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4257 - val_loss: 624.5655\n",
      "Epoch 688/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.4051 - val_loss: 622.2576\n",
      "Epoch 689/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3987 - val_loss: 620.7083\n",
      "Epoch 690/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4319 - val_loss: 621.6337\n",
      "Epoch 691/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4512 - val_loss: 619.6524\n",
      "Epoch 692/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4162 - val_loss: 622.8838\n",
      "Epoch 693/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3579 - val_loss: 618.9786\n",
      "Epoch 694/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3524 - val_loss: 622.5916\n",
      "Epoch 695/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3809 - val_loss: 622.6930\n",
      "Epoch 696/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4645 - val_loss: 622.5291\n",
      "Epoch 697/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4261 - val_loss: 627.4947\n",
      "Epoch 698/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3359 - val_loss: 624.9671\n",
      "Epoch 699/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3559 - val_loss: 626.0202\n",
      "Epoch 700/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.3306 - val_loss: 627.1144\n",
      "Epoch 701/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3633 - val_loss: 626.8028\n",
      "Epoch 702/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.3037 - val_loss: 622.3818\n",
      "Epoch 703/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 32.3025 - val_loss: 625.1180\n",
      "Epoch 704/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2706 - val_loss: 622.2341\n",
      "Epoch 705/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2788 - val_loss: 623.9909\n",
      "Epoch 706/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3549 - val_loss: 627.1671\n",
      "Epoch 707/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.2729 - val_loss: 625.4670\n",
      "Epoch 708/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.2621 - val_loss: 624.8815\n",
      "Epoch 709/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3574 - val_loss: 625.9261\n",
      "Epoch 710/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2896 - val_loss: 630.0218\n",
      "Epoch 711/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.2985 - val_loss: 626.1738\n",
      "Epoch 712/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.4182 - val_loss: 624.1669\n",
      "Epoch 713/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2850 - val_loss: 628.0390\n",
      "Epoch 714/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.2677 - val_loss: 627.4938\n",
      "Epoch 715/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.2545 - val_loss: 625.4575\n",
      "Epoch 716/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.3204 - val_loss: 628.2123\n",
      "Epoch 717/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3064 - val_loss: 628.7990\n",
      "Epoch 718/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.2057 - val_loss: 628.3799\n",
      "Epoch 719/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2331 - val_loss: 628.5999\n",
      "Epoch 720/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.1672 - val_loss: 624.5510\n",
      "Epoch 721/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3259 - val_loss: 630.2775\n",
      "Epoch 722/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3241 - val_loss: 626.8841\n",
      "Epoch 723/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1684 - val_loss: 626.5631\n",
      "Epoch 724/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.2444 - val_loss: 627.5450\n",
      "Epoch 725/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.1888 - val_loss: 630.0255\n",
      "Epoch 726/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1754 - val_loss: 635.3561\n",
      "Epoch 727/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2171 - val_loss: 635.3686\n",
      "Epoch 728/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.1348 - val_loss: 631.2382\n",
      "Epoch 729/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.2565 - val_loss: 634.8029\n",
      "Epoch 730/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.1297 - val_loss: 631.9682\n",
      "Epoch 731/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1508 - val_loss: 633.1673\n",
      "Epoch 732/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1033 - val_loss: 635.1744\n",
      "Epoch 733/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1675 - val_loss: 633.8232\n",
      "Epoch 734/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1406 - val_loss: 638.6958\n",
      "Epoch 735/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0966 - val_loss: 636.0665\n",
      "Epoch 736/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1165 - val_loss: 641.1855\n",
      "Epoch 737/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2943 - val_loss: 645.2382\n",
      "Epoch 738/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1710 - val_loss: 640.6998\n",
      "Epoch 739/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0811 - val_loss: 639.1690\n",
      "Epoch 740/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0926 - val_loss: 638.3370\n",
      "Epoch 741/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1331 - val_loss: 642.5679\n",
      "Epoch 742/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.0776 - val_loss: 639.6609\n",
      "Epoch 743/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1997 - val_loss: 639.1990\n",
      "Epoch 744/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.0318 - val_loss: 640.5613\n",
      "Epoch 745/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0894 - val_loss: 636.7090\n",
      "Epoch 746/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.0249 - val_loss: 638.7745\n",
      "Epoch 747/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0387 - val_loss: 637.7849\n",
      "Epoch 748/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.0152 - val_loss: 640.7645\n",
      "Epoch 749/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.1032 - val_loss: 640.3549\n",
      "Epoch 750/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.0095 - val_loss: 646.2610\n",
      "Epoch 751/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.9864 - val_loss: 642.0802\n",
      "Epoch 752/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2317 - val_loss: 645.0806\n",
      "Epoch 753/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.0504 - val_loss: 645.5043\n",
      "Epoch 754/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0121 - val_loss: 643.5464\n",
      "Epoch 755/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.9523 - val_loss: 641.0336\n",
      "Epoch 756/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.9735 - val_loss: 645.1719\n",
      "Epoch 757/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.0237 - val_loss: 641.9834\n",
      "Epoch 758/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.9344 - val_loss: 643.6568\n",
      "Epoch 759/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.9000 - val_loss: 640.8265\n",
      "Epoch 760/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.9353 - val_loss: 639.2915\n",
      "Epoch 761/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0177 - val_loss: 645.5675\n",
      "Epoch 762/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.9631 - val_loss: 643.7562\n",
      "Epoch 763/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.9954 - val_loss: 643.3179\n",
      "Epoch 764/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.9917 - val_loss: 647.0433\n",
      "Epoch 765/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.9360 - val_loss: 645.5462\n",
      "Epoch 766/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8860 - val_loss: 645.2211\n",
      "Epoch 767/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.9816 - val_loss: 642.7263\n",
      "Epoch 768/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0496 - val_loss: 647.9785\n",
      "Epoch 769/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8677 - val_loss: 643.6195\n",
      "Epoch 770/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.0168 - val_loss: 645.2151\n",
      "Epoch 771/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8686 - val_loss: 642.6497\n",
      "Epoch 772/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8534 - val_loss: 644.9370\n",
      "Epoch 773/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8654 - val_loss: 646.3737\n",
      "Epoch 774/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8394 - val_loss: 645.6948\n",
      "Epoch 775/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8466 - val_loss: 647.4684\n",
      "Epoch 776/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8291 - val_loss: 642.1436\n",
      "Epoch 777/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8742 - val_loss: 649.1411\n",
      "Epoch 778/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8773 - val_loss: 642.1226\n",
      "Epoch 779/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.7689 - val_loss: 652.8806\n",
      "Epoch 780/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32.0514 - val_loss: 649.1638\n",
      "Epoch 781/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 32.2155 - val_loss: 652.5083\n",
      "Epoch 782/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8140 - val_loss: 648.8625\n",
      "Epoch 783/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8711 - val_loss: 651.3198\n",
      "Epoch 784/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8941 - val_loss: 653.9525\n",
      "Epoch 785/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8804 - val_loss: 651.3812\n",
      "Epoch 786/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8604 - val_loss: 654.2219\n",
      "Epoch 787/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8598 - val_loss: 647.9843\n",
      "Epoch 788/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8280 - val_loss: 657.5266\n",
      "Epoch 789/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6889 - val_loss: 651.5148\n",
      "Epoch 790/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.9074 - val_loss: 656.6878\n",
      "Epoch 791/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.7006 - val_loss: 652.8333\n",
      "Epoch 792/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.7435 - val_loss: 655.7106\n",
      "Epoch 793/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.7335 - val_loss: 656.4283\n",
      "Epoch 794/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8250 - val_loss: 655.0399\n",
      "Epoch 795/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.7768 - val_loss: 656.0308\n",
      "Epoch 796/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.7973 - val_loss: 652.7678\n",
      "Epoch 797/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.7582 - val_loss: 653.1596\n",
      "Epoch 798/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.7448 - val_loss: 653.0545\n",
      "Epoch 799/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6907 - val_loss: 653.8664\n",
      "Epoch 800/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6528 - val_loss: 657.1116\n",
      "Epoch 801/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.7047 - val_loss: 655.3757\n",
      "Epoch 802/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.7489 - val_loss: 661.3189\n",
      "Epoch 803/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.8199 - val_loss: 658.2930\n",
      "Epoch 804/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.6595 - val_loss: 657.3621\n",
      "Epoch 805/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.7461 - val_loss: 656.5233\n",
      "Epoch 806/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.8619 - val_loss: 660.0172\n",
      "Epoch 807/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.6025 - val_loss: 650.7882\n",
      "Epoch 808/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.7774 - val_loss: 656.5685\n",
      "Epoch 809/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.6356 - val_loss: 654.1019\n",
      "Epoch 810/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6545 - val_loss: 656.5746\n",
      "Epoch 811/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.7021 - val_loss: 655.5107\n",
      "Epoch 812/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6515 - val_loss: 654.8669\n",
      "Epoch 813/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.7321 - val_loss: 660.0949\n",
      "Epoch 814/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.7521 - val_loss: 657.4899\n",
      "Epoch 815/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.6639 - val_loss: 657.9957\n",
      "Epoch 816/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6643 - val_loss: 659.7344\n",
      "Epoch 817/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5649 - val_loss: 657.3746\n",
      "Epoch 818/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5820 - val_loss: 656.4655\n",
      "Epoch 819/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6098 - val_loss: 663.4900\n",
      "Epoch 820/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6837 - val_loss: 662.7919\n",
      "Epoch 821/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5984 - val_loss: 664.7501\n",
      "Epoch 822/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.5399 - val_loss: 663.2529\n",
      "Epoch 823/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.5678 - val_loss: 665.6638\n",
      "Epoch 824/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.5485 - val_loss: 665.4550\n",
      "Epoch 825/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.5376 - val_loss: 661.3217\n",
      "Epoch 826/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4798 - val_loss: 665.9667\n",
      "Epoch 827/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5497 - val_loss: 667.2407\n",
      "Epoch 828/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5751 - val_loss: 669.6905\n",
      "Epoch 829/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4709 - val_loss: 664.8082\n",
      "Epoch 830/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5655 - val_loss: 668.1885\n",
      "Epoch 831/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4756 - val_loss: 664.1871\n",
      "Epoch 832/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.4542 - val_loss: 663.1736\n",
      "Epoch 833/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5883 - val_loss: 659.3362\n",
      "Epoch 834/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4506 - val_loss: 664.1396\n",
      "Epoch 835/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4528 - val_loss: 664.5782\n",
      "Epoch 836/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4172 - val_loss: 659.9466\n",
      "Epoch 837/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5105 - val_loss: 665.8527\n",
      "Epoch 838/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4769 - val_loss: 664.4092\n",
      "Epoch 839/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4795 - val_loss: 663.6939\n",
      "Epoch 840/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.5455 - val_loss: 669.8271\n",
      "Epoch 841/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6597 - val_loss: 662.4734\n",
      "Epoch 842/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4281 - val_loss: 663.9870\n",
      "Epoch 843/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5249 - val_loss: 670.9990\n",
      "Epoch 844/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3277 - val_loss: 664.9227\n",
      "Epoch 845/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4278 - val_loss: 670.5046\n",
      "Epoch 846/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.4680 - val_loss: 670.6136\n",
      "Epoch 847/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3837 - val_loss: 667.9798\n",
      "Epoch 848/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3546 - val_loss: 666.9733\n",
      "Epoch 849/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3323 - val_loss: 669.4298\n",
      "Epoch 850/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3429 - val_loss: 669.1801\n",
      "Epoch 851/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3629 - val_loss: 671.9169\n",
      "Epoch 852/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.4208 - val_loss: 667.6982\n",
      "Epoch 853/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.5170 - val_loss: 667.9337\n",
      "Epoch 854/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6644 - val_loss: 671.7994\n",
      "Epoch 855/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3962 - val_loss: 667.4832\n",
      "Epoch 856/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3641 - val_loss: 670.8144\n",
      "Epoch 857/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3078 - val_loss: 668.6996\n",
      "Epoch 858/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3023 - val_loss: 668.2684\n",
      "Epoch 859/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2581 - val_loss: 677.3862\n",
      "Epoch 860/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.4842 - val_loss: 669.3325\n",
      "Epoch 861/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.5594 - val_loss: 672.6301\n",
      "Epoch 862/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3364 - val_loss: 675.0005\n",
      "Epoch 863/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3783 - val_loss: 673.6802\n",
      "Epoch 864/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.5402 - val_loss: 670.1270\n",
      "Epoch 865/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2943 - val_loss: 678.5881\n",
      "Epoch 866/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.4206 - val_loss: 672.0779\n",
      "Epoch 867/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2766 - val_loss: 674.8038\n",
      "Epoch 868/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2585 - val_loss: 672.3010\n",
      "Epoch 869/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3411 - val_loss: 671.1739\n",
      "Epoch 870/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2963 - val_loss: 671.6502\n",
      "Epoch 871/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2714 - val_loss: 667.2122\n",
      "Epoch 872/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3037 - val_loss: 671.1047\n",
      "Epoch 873/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2618 - val_loss: 663.7742\n",
      "Epoch 874/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3512 - val_loss: 669.8500\n",
      "Epoch 875/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2732 - val_loss: 669.2516\n",
      "Epoch 876/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2229 - val_loss: 671.5738\n",
      "Epoch 877/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3229 - val_loss: 669.9025\n",
      "Epoch 878/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2616 - val_loss: 669.9173\n",
      "Epoch 879/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1972 - val_loss: 672.4861\n",
      "Epoch 880/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2121 - val_loss: 668.0059\n",
      "Epoch 881/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2690 - val_loss: 664.8517\n",
      "Epoch 882/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1906 - val_loss: 669.5302\n",
      "Epoch 883/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2267 - val_loss: 669.8566\n",
      "Epoch 884/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1980 - val_loss: 672.9672\n",
      "Epoch 885/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2336 - val_loss: 670.8832\n",
      "Epoch 886/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2485 - val_loss: 668.9218\n",
      "Epoch 887/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2153 - val_loss: 671.6430\n",
      "Epoch 888/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.4092 - val_loss: 668.0165\n",
      "Epoch 889/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1947 - val_loss: 669.0435\n",
      "Epoch 890/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1709 - val_loss: 669.0239\n",
      "Epoch 891/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2030 - val_loss: 664.4485\n",
      "Epoch 892/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2465 - val_loss: 667.0527\n",
      "Epoch 893/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2821 - val_loss: 672.2548\n",
      "Epoch 894/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2750 - val_loss: 662.8199\n",
      "Epoch 895/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2700 - val_loss: 673.0698\n",
      "Epoch 896/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2471 - val_loss: 667.7945\n",
      "Epoch 897/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.4378 - val_loss: 670.4883\n",
      "Epoch 898/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.4255 - val_loss: 668.9139\n",
      "Epoch 899/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1780 - val_loss: 666.0167\n",
      "Epoch 900/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1779 - val_loss: 671.2305\n",
      "Epoch 901/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2912 - val_loss: 668.8239\n",
      "Epoch 902/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3116 - val_loss: 677.6587\n",
      "Epoch 903/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3794 - val_loss: 670.2993\n",
      "Epoch 904/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1493 - val_loss: 669.2022\n",
      "Epoch 905/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1327 - val_loss: 667.5617\n",
      "Epoch 906/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2815 - val_loss: 672.9374\n",
      "Epoch 907/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1467 - val_loss: 665.5535\n",
      "Epoch 908/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3514 - val_loss: 674.0389\n",
      "Epoch 909/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3964 - val_loss: 667.4189\n",
      "Epoch 910/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1236 - val_loss: 668.5849\n",
      "Epoch 911/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1313 - val_loss: 670.1785\n",
      "Epoch 912/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1858 - val_loss: 672.0605\n",
      "Epoch 913/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1292 - val_loss: 670.7402\n",
      "Epoch 914/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1115 - val_loss: 670.7873\n",
      "Epoch 915/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1010 - val_loss: 670.8087\n",
      "Epoch 916/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1433 - val_loss: 668.3397\n",
      "Epoch 917/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1794 - val_loss: 674.8185\n",
      "Epoch 918/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3569 - val_loss: 668.5264\n",
      "Epoch 919/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0882 - val_loss: 672.1226\n",
      "Epoch 920/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0885 - val_loss: 669.1143\n",
      "Epoch 921/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1942 - val_loss: 676.8605\n",
      "Epoch 922/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1455 - val_loss: 672.4966\n",
      "Epoch 923/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1514 - val_loss: 677.8895\n",
      "Epoch 924/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2934 - val_loss: 671.5115\n",
      "Epoch 925/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0978 - val_loss: 671.7655\n",
      "Epoch 926/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0978 - val_loss: 671.0152\n",
      "Epoch 927/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1908 - val_loss: 668.2504\n",
      "Epoch 928/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1579 - val_loss: 672.8394\n",
      "Epoch 929/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.3532 - val_loss: 671.6325\n",
      "Epoch 930/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1029 - val_loss: 673.9041\n",
      "Epoch 931/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0546 - val_loss: 667.8698\n",
      "Epoch 932/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1202 - val_loss: 675.8092\n",
      "Epoch 933/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1004 - val_loss: 668.5097\n",
      "Epoch 934/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1124 - val_loss: 670.0783\n",
      "Epoch 935/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1764 - val_loss: 670.6607\n",
      "Epoch 936/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.2990 - val_loss: 667.9130\n",
      "Epoch 937/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3021 - val_loss: 670.8137\n",
      "Epoch 938/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0276 - val_loss: 669.1121\n",
      "Epoch 939/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0539 - val_loss: 669.4356\n",
      "Epoch 940/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0662 - val_loss: 667.8410\n",
      "Epoch 941/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0975 - val_loss: 675.7003\n",
      "Epoch 942/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0606 - val_loss: 669.8215\n",
      "Epoch 943/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1138 - val_loss: 676.7472\n",
      "Epoch 944/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1537 - val_loss: 670.3467\n",
      "Epoch 945/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0420 - val_loss: 672.8931\n",
      "Epoch 946/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0220 - val_loss: 670.2324\n",
      "Epoch 947/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1732 - val_loss: 673.9177\n",
      "Epoch 948/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2170 - val_loss: 669.6857\n",
      "Epoch 949/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0276 - val_loss: 666.3445\n",
      "Epoch 950/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9825 - val_loss: 671.0639\n",
      "Epoch 951/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2526 - val_loss: 667.6944\n",
      "Epoch 952/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.1487 - val_loss: 672.0403\n",
      "Epoch 953/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0335 - val_loss: 670.4899\n",
      "Epoch 954/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0206 - val_loss: 670.9755\n",
      "Epoch 955/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0056 - val_loss: 667.4702\n",
      "Epoch 956/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0609 - val_loss: 669.8307\n",
      "Epoch 957/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0215 - val_loss: 666.3046\n",
      "Epoch 958/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0183 - val_loss: 669.3134\n",
      "Epoch 959/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.2600 - val_loss: 670.6982\n",
      "Epoch 960/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9991 - val_loss: 671.4088\n",
      "Epoch 961/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0269 - val_loss: 670.0621\n",
      "Epoch 962/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0914 - val_loss: 671.9051\n",
      "Epoch 963/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0312 - val_loss: 672.4639\n",
      "Epoch 964/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0664 - val_loss: 668.7287\n",
      "Epoch 965/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0625 - val_loss: 672.3026\n",
      "Epoch 966/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0674 - val_loss: 671.4228\n",
      "Epoch 967/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 31.0024 - val_loss: 669.9218\n",
      "Epoch 968/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9745 - val_loss: 671.4333\n",
      "Epoch 969/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.9521 - val_loss: 671.4428\n",
      "Epoch 970/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9745 - val_loss: 671.3586\n",
      "Epoch 971/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9421 - val_loss: 670.2419\n",
      "Epoch 972/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9633 - val_loss: 671.6645\n",
      "Epoch 973/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.1212 - val_loss: 668.3708\n",
      "Epoch 974/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.3035 - val_loss: 671.1738\n",
      "Epoch 975/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9687 - val_loss: 669.4620\n",
      "Epoch 976/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9281 - val_loss: 673.4402\n",
      "Epoch 977/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9910 - val_loss: 674.2650\n",
      "Epoch 978/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9628 - val_loss: 673.1473\n",
      "Epoch 979/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9764 - val_loss: 667.4527\n",
      "Epoch 980/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9819 - val_loss: 669.5839\n",
      "Epoch 981/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.9873 - val_loss: 668.3269\n",
      "Epoch 982/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9530 - val_loss: 670.7701\n",
      "Epoch 983/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0163 - val_loss: 669.1737\n",
      "Epoch 984/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.9614 - val_loss: 666.7300\n",
      "Epoch 985/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9083 - val_loss: 672.4472\n",
      "Epoch 986/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9323 - val_loss: 665.9941\n",
      "Epoch 987/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0587 - val_loss: 669.2053\n",
      "Epoch 988/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.9883 - val_loss: 670.2187\n",
      "Epoch 989/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9795 - val_loss: 669.7816\n",
      "Epoch 990/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9016 - val_loss: 665.4238\n",
      "Epoch 991/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.9071 - val_loss: 670.2065\n",
      "Epoch 992/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9052 - val_loss: 668.3193\n",
      "Epoch 993/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.8807 - val_loss: 668.9133\n",
      "Epoch 994/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.8899 - val_loss: 670.6140\n",
      "Epoch 995/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.0286 - val_loss: 667.2960\n",
      "Epoch 996/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.9843 - val_loss: 671.3515\n",
      "Epoch 997/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9382 - val_loss: 664.0670\n",
      "Epoch 998/1000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9926 - val_loss: 670.5437\n",
      "Epoch 999/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.9531 - val_loss: 668.7209\n",
      "Epoch 1000/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.8960 - val_loss: 666.7482\n"
     ]
    }
   ],
   "source": [
    "# Initiating the optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=losses.MeanAbsoluteError(), optimizer=optimizer)\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(train_X, train_y, epochs = 1000, batch_size=16, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(mse))\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(epochs, mse, 'r', label='Training loss', color='blue')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss', color='red')\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
